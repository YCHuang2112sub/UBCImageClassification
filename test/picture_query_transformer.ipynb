{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REF\n",
    "\n",
    "- https://theaisummer.com/unet-architectures/\n",
    "- [CTRANSCNN](https://www.sciencedirect.com/science/article/pii/S0950705123007803)\n",
    "- [CrossViT]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import models\n",
    "from torchsummary import summary\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import validators\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 3, 2, 2])\n",
      "torch.Size([1, 9, 12])\n"
     ]
    }
   ],
   "source": [
    "# X = torch.randn(1, 3, 512, 512)\n",
    "# # X = nn.max_pool2d(X, kernel_size=2, stride=2)\n",
    "\n",
    "\n",
    "# # torch.tensor_split(X, (16,16), dim=(2,3))\n",
    "# X = torch.tensor_split(X.flatten(-2), 16*16)\n",
    "# print(X[0].shape)\n",
    "# print(X.shape)\n",
    "\n",
    "# S = 128 # channel dim\n",
    "# W = 227 # width\n",
    "# H = 227 # height\n",
    "# batch_size = 10\n",
    "\n",
    "# image_new = X\n",
    "\n",
    "# x = image_new.unsqueeze(0)\n",
    "\n",
    "# size = 32 # patch size\n",
    "# stride = 32 # patch stride\n",
    "# patches = x.unfold(1, size, stride).unfold(2, size, stride).unfold(3, size, stride)\n",
    "# print(patches.shape)\n",
    "\n",
    "def split2SubImages(imgs: torch.tensor, subimage_size: tuple):\n",
    "    imgs_shape = imgs.shape\n",
    "    assert(len(imgs_shape) == 4) # B * C * Hi * Wi \n",
    "    assert(len(subimage_size) == 2) # Ho * Wo\n",
    "    # check fully divisible\n",
    "    assert( (imgs_shape[2] % subimage_size[0] == 0) and (imgs_shape[3] % subimage_size[1] == 0))\n",
    "    \n",
    "    # print(imgs)\n",
    "    # print(subimage_size)\n",
    "    # a1 = torch.nn.Unfold(kernel_size=subimage_size, dilation=subimage_size, stride=(1,1))(imgs)\n",
    "    # a1 = torch.nn.Unfold(kernel_size=subimage_size)(imgs)\n",
    "    a1 = torch.nn.Unfold(subimage_size, dilation=(1,1), stride=subimage_size)(imgs)\n",
    "    # print(a1.shape)\n",
    "    # print(imgs_shape)\n",
    "    # print(a1)\n",
    "    B = imgs.shape[0]\n",
    "    C = imgs.shape[1]\n",
    "    L = a1.shape[-1]\n",
    "    a2 = a1.permute(0,2,1) # B * L * (C x Ho x Wo)\n",
    "    a3 = a2.reshape((B,L,C)+subimage_size) \n",
    "\n",
    "    out_shape = a3.shape # B * num_imgs * C * Ho * Wo\n",
    "\n",
    "    return a3 # B * num_imgs * C * Ho * Wo\n",
    "    print(out_shape)\n",
    "    # print(X.shape[:-1] + (2,2))\n",
    "    # print(a1.shape)\n",
    "    # #print(a1)\n",
    "    # print(a2.shape)\n",
    "    # print(a2)\n",
    "    # print(a3.shape)\n",
    "    # print(a3)\n",
    "    # print(a4.shape)\n",
    "    # print(a4)\n",
    "\n",
    "imgs = torch.arange(6*6*3).reshape(1, 3, 6, 6).float()\n",
    "# imgs = torch.randn(1, 3, 512, 512)\n",
    "# print(imgs)\n",
    "subimage_size = (2,2)\n",
    "sub_imgs = split2SubImages(imgs, subimage_size)\n",
    "# print(sub_imgs)\n",
    "print(sub_imgs.shape)\n",
    "keys = sub_imgs.reshape(sub_imgs.shape[:2] + (-1,)) # B * num_imgs * (CxHxW)\n",
    "print(keys.shape )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 768])\n",
      "\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 256, 768])\n",
      "\n",
      "torch.Size([1, 3, 128, 128])\n",
      "torch.Size([1, 64, 768])\n",
      "\n",
      "torch.Size([1, 3, 64, 64])\n",
      "torch.Size([1, 16, 768])\n",
      "\n",
      "torch.Size([1, 3, 32, 32])\n",
      "torch.Size([1, 4, 768])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.randn(1, 3, 512, 512)\n",
    "subimage_size = (16,16)\n",
    "img_t01 = split2SubImages(imgs, subimage_size)\n",
    "k_t01 = img_t01.reshape(img_t01.shape[:2] + (-1,))\n",
    "# print(sub_imgs.shape)\n",
    "print(k_t01.shape)\n",
    "\n",
    "print()\n",
    "x2 = nn.MaxPool2d(kernel_size=2, stride=2)(imgs)\n",
    "print(x2.shape)\n",
    "\n",
    "img_t02 = split2SubImages(x2, subimage_size)\n",
    "k_t02 = img_t02.reshape(img_t02.shape[:2] + (-1,))\n",
    "print(k_t02.shape)\n",
    "\n",
    "print()\n",
    "x3 = nn.MaxPool2d(kernel_size=2, stride=2)(x2)\n",
    "print(x3.shape)\n",
    "\n",
    "img_t03 = split2SubImages(x3, subimage_size)\n",
    "k_t03 = img_t03.reshape(img_t03.shape[:2] + (-1,))\n",
    "print(k_t03.shape)\n",
    "\n",
    "\n",
    "print()\n",
    "x4 = nn.MaxPool2d(kernel_size=2, stride=2)(x3)\n",
    "print(x4.shape)\n",
    "\n",
    "img_t04 = split2SubImages(x4, subimage_size)\n",
    "k_t04 = img_t04.reshape(img_t04.shape[:2] + (-1,))\n",
    "print(k_t04.shape)\n",
    "\n",
    "\n",
    "print()\n",
    "x5 = nn.MaxPool2d(kernel_size=2, stride=2)(x4)\n",
    "print(x5.shape)\n",
    "\n",
    "img_t05 = split2SubImages(x5, subimage_size)\n",
    "k_t05 = img_t05.reshape(img_t05.shape[:2] + (-1,))\n",
    "print(k_t05.shape)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "imgs = torch.randn(1, 16, 64, 64)\n",
    "subimage_size = (16,16)\n",
    "img_t01 = split2SubImages(imgs, subimage_size) # B * num_imgs * C * Ho * Wo\n",
    "B, num_imgs, C, Ho, Wo = img_t01.shape\n",
    "k_t01 = img_t01.reshape((B, num_imgs*C, Ho*Wo))\n",
    "# print(sub_imgs.shape)\n",
    "print(k_t01.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownscaleCNN(nn.Module):\n",
    "    def __init__(self, in_ch=3, out_ch=16):\n",
    "        super(DownscaleCNN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pooling = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.relu(x1)\n",
    "\n",
    "        x2 = self.max_pooling(x1)\n",
    "        x2 = self.conv2(x2)\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.relu(x2)\n",
    "\n",
    "        x3 = self.max_pooling(x2)\n",
    "        x3 = self.conv3(x3)\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.relu(x3)\n",
    "    \n",
    "        x4 = self.max_pooling(x3)\n",
    "        x4 = self.conv4(x4)\n",
    "        x4 = self.bn4(x4)\n",
    "        x4 = self.relu(x4)\n",
    "    \n",
    "        return x4, (x1, x2, x3, x4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "torch.Size([1, 16, 512, 512])\n",
      "torch.Size([1, 16, 256, 256])\n",
      "torch.Size([1, 16, 128, 128])\n",
      "torch.Size([1, 16, 64, 64])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 512, 512]             448\n",
      "       BatchNorm2d-2         [-1, 16, 512, 512]              32\n",
      "              ReLU-3         [-1, 16, 512, 512]               0\n",
      "         MaxPool2d-4         [-1, 16, 256, 256]               0\n",
      "            Conv2d-5         [-1, 16, 256, 256]           2,320\n",
      "       BatchNorm2d-6         [-1, 16, 256, 256]              32\n",
      "              ReLU-7         [-1, 16, 256, 256]               0\n",
      "         MaxPool2d-8         [-1, 16, 128, 128]               0\n",
      "            Conv2d-9         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-10         [-1, 16, 128, 128]              32\n",
      "             ReLU-11         [-1, 16, 128, 128]               0\n",
      "        MaxPool2d-12           [-1, 16, 64, 64]               0\n",
      "           Conv2d-13           [-1, 16, 64, 64]           2,320\n",
      "      BatchNorm2d-14           [-1, 16, 64, 64]              32\n",
      "             ReLU-15           [-1, 16, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 7,536\n",
      "Trainable params: 7,536\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 138.00\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 141.03\n",
      "----------------------------------------------------------------\n",
      "\n",
      "torch.Size([1, 16384, 256])\n",
      "\n",
      "torch.Size([1, 4096, 256])\n",
      "\n",
      "torch.Size([1, 1024, 256])\n",
      "\n",
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "downscale_cnn = DownscaleCNN(in_ch=3, out_ch=16)\n",
    "imgs = torch.randn(1, 3, 512, 512)\n",
    "_, (x1, x2, x3, x4) = downscale_cnn(imgs)\n",
    "print(imgs.shape)\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "print(x4.shape)\n",
    "\n",
    "# summary(downscale_cnn.to(device), (3,512,512), device=device.type)\n",
    "\n",
    "\n",
    "\n",
    "subimage_size = (16,16)\n",
    "\n",
    "for x in [x1, x2, x3, x4]:\n",
    "    print()\n",
    "    img_t01 = split2SubImages(x, subimage_size) # B * num_imgs * C * Ho * Wo\n",
    "    B, num_imgs, C, Ho, Wo = img_t01.shape\n",
    "    k_t01 = img_t01.reshape((B, num_imgs*C, Ho*Wo))\n",
    "    # print(sub_imgs.shape)\n",
    "    print(k_t01.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead_attn = nn.MultiheadAttention(128, 8, batch_first=True)\n",
    "# query = torch.randn(10, 32, 128)\n",
    "# key = query\n",
    "# value = torch.randn(10, 32, 128)\n",
    "# attn_output, attn_output_weights = multihead_attn(query, key, value, average_attn_weights=False)\n",
    "\n",
    "# print(attn_output.shape)\n",
    "# print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        # self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        \n",
    "        self.norm1_q = nn.LayerNorm(hidden_d)\n",
    "        self.norm1_k = nn.LayerNorm(hidden_d)\n",
    "        self.norm1_v = nn.LayerNorm(hidden_d)\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_d, n_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # x = v\n",
    "        # out = x + self.mhsa(self.norm1(x))\n",
    "        q1, k1, v1 = self.norm1_q(q), self.norm1_k(k), self.norm1_v(v)\n",
    "        attn_output, attn_output_weights = self.multihead_attn(q1, k1, v1, average_attn_weights=False)\n",
    "        out = v + attn_output\n",
    "        \n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self, data_dim=(256,512), hidden_d=512, n_heads=16, out_d=5) -> None:\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        \n",
    "        assert(len(data_dim) == 2)\n",
    "\n",
    "        self.data_dim = data_dim # (Length, embedding_dim)\n",
    "        self.seq_len, self.emb_dim = data_dim\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.out_d = out_d\n",
    "\n",
    "        # 1) Input transformation\n",
    "        self.linear_mapper = nn.Linear(data_dim[1], hidden_d)\n",
    "\n",
    "        # 2) Positional embedding\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.seq_len, hidden_d))\n",
    "        # self.pos_embeddings = self.get_positional_embeddings(self.seq_len, hidden_d)\n",
    "\n",
    "\n",
    "        # 3) Transformer encoder blocks\n",
    "        # self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        self.block0 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block1 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block2 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block3 = MyViTBlock(hidden_d, n_heads)\n",
    "\n",
    "        # 4) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, hidden_d),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            # nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x5, v1, v2):\n",
    "        B_size = x5.shape[0]\n",
    "\n",
    "        tokens = self.linear_mapper(x5)\n",
    "        \n",
    "        # Adding classification token to the tokens\n",
    "        # tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        pos_embed = self.positional_embeddings.repeat(B_size, 1, 1)\n",
    "        out = tokens + pos_embed  \n",
    "        \n",
    "        # Transformer Blocks\n",
    "        # for block in self.blocks:\n",
    "        #     out2 = block(out, out, out)\n",
    "            \n",
    "        #     out = out2\n",
    "        x_stage1 = self.block0(out, v1, v1) # Q, K, V\n",
    "        x_stage2 = self.block1(x_stage1, x_stage1, x_stage1)\n",
    "        x_stage3 = self.block2(x_stage2, v2, v2)\n",
    "        x_stage4 = self.block3(x_stage3, x_stage3, x_stage3)\n",
    "\n",
    "        out = x_stage4\n",
    "            \n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out), out # Map to output dimension, output category distribution\n",
    "            \n",
    "    \n",
    "    def get_positional_embeddings(self, sequence_length, d):\n",
    "        result = torch.ones(sequence_length, d)\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(d):\n",
    "                result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyViT(nn.Module):\n",
    "#     def __init__(self, chw=(1, 28, 28), n_patches=7, n_blocks=2, hidden_d=128, n_heads=2, out_d=10):\n",
    "#         # Super constructor\n",
    "#         super(MyViT, self).__init__()\n",
    "\n",
    "#         # Attributes\n",
    "#         self.chw = chw # (C, H, W)\n",
    "#         self.n_patches = n_patches\n",
    "#         self.n_blocks = n_blocks\n",
    "#         self.n_heads = n_heads\n",
    "#         self.hidden_d = hidden_d\n",
    "\n",
    "#         assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "#         assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "#         self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "#         # 1) Linear mapper\n",
    "#         self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "#         self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "#         # 2) Learnable classifiation token\n",
    "#         self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "        \n",
    "#         # # 3) Positional embedding\n",
    "#         # self.positional_embeddings = nn.Parameter(torch.tensor(self.get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))\n",
    "#         # self.positional_embeddings.requires_grad = False\n",
    "        \n",
    "        \n",
    "#         # 3) Positional embedding\n",
    "#         self.register_buffer('positional_embeddings', self.get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "#         # 4) Transformer encoder blocks\n",
    "#         self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "           \n",
    "#         # 5) Classification MLPk\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(self.hidden_d, out_d),\n",
    "#             nn.Softmax(dim=-1)\n",
    "#         )\n",
    "        \n",
    "\n",
    "#     def forward(self, images):\n",
    "#         n, c, h, w = images.shape\n",
    "#         patches = self.patchify(images, self.n_patches)\n",
    "#         # print(patches)\n",
    "#         tokens = self.linear_mapper(patches)\n",
    "        \n",
    "#         # Adding classification token to the tokens\n",
    "#         tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "        \n",
    "#         # Adding positional embedding\n",
    "#         pos_embed = self.positional_embeddings.repeat(n, 1, 1)\n",
    "#         out = tokens + pos_embed  \n",
    "        \n",
    "#         # Transformer Blocks\n",
    "#         for block in self.blocks:\n",
    "#             out2 = block(out, out, out)\n",
    "            \n",
    "#             out = out2\n",
    "            \n",
    "#         # Getting the classification token only\n",
    "#         out = out[:, 0]\n",
    "        \n",
    "#         return self.mlp(out), out # Map to output dimension, output category distribution\n",
    "            \n",
    "\n",
    "#     def patchify(self, images, n_patches):\n",
    "#         assert len(images.shape) == 4\n",
    "#         b,c,h,w = images.shape\n",
    "\n",
    "#         size_h = int(h/n_patches) # patch size\n",
    "#         stride_h = size_h # patch stride\n",
    "#         size_w = int(w/n_patches) # patch size\n",
    "#         stride_w = size_w # patch stride\n",
    "#         x = images.permute((0,2,3,1))\n",
    "#         x2 = x.unfold(1, size_h, stride_h).unfold(2, size_w, stride_w).flatten(start_dim=1, end_dim=2)\n",
    "#         # print(x2.shape, \"x2 pachify\") # x2.shape = (b,p**2, c,h,w)\n",
    "#         x3 = x2.permute((0,1,3,4,2)).flatten(start_dim=2, end_dim=4)\n",
    "#         # print(x3.shape, \"x3 pachify\") # x3.shape = (b,p**2,h*w*c)\n",
    "        \n",
    "#         return x3\n",
    "\n",
    "#     def get_positional_embeddings(self, sequence_length, d):\n",
    "#         result = torch.ones(sequence_length, d)\n",
    "#         for i in range(sequence_length):\n",
    "#             for j in range(d):\n",
    "#                 result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "#         return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/../')\n",
    "from lib.network_architecture.unet_transformer_01 import MyViTBlock, FeatureTransformer, Unet, BridgingModel, \\\n",
    "                                                         get_unet_transformer_model_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = Unet(3, 3).to(device)\n",
    "bridging_model = BridgingModel().to(device)\n",
    "feature_transformer_model = FeatureTransformer(data_dim=(256,512), hidden_d=512, n_heads=16, out_d=5).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1291,  0.1260, -0.1129, -0.1690, -0.0675]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([1], device='cuda:0')\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 3, 256, 256).to(device)\n",
    "# out, hidden_features = unet_model(X)\n",
    "# # (x1, x2, x3, x4, x5) = hidden_features\n",
    "# fs = bridging_model(*hidden_features)\n",
    "# B, C, H, W = fs[0].shape\n",
    "# vs = [x.permute(0,2,3,1).reshape(B, H*W, C) for x in fs]\n",
    "\n",
    "\n",
    "# for v in vs:\n",
    "#     print(v.shape)\n",
    "\n",
    "\n",
    "# x5 = hidden_features[4].permute(0,2,3,1).reshape(B, H*W, C)\n",
    "# print(x5.shape)\n",
    "\n",
    "# y_pred, out = feature_transformer_model(x5, vs[0], vs[1])\n",
    "\n",
    "y_pred, out, unet_out = get_unet_transformer_model_output(X, unet_model, bridging_model, feature_transformer_model)\n",
    "\n",
    "print(y_pred, torch.argmax(y_pred, dim=-1))\n",
    "print(unet_out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
      "              ReLU-3         [-1, 64, 512, 512]               0\n",
      "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
      "              ReLU-6         [-1, 64, 512, 512]               0\n",
      "        DoubleConv-7         [-1, 64, 512, 512]               0\n",
      "            InConv-8         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-9         [-1, 64, 256, 256]               0\n",
      "           Conv2d-10        [-1, 128, 256, 256]          73,856\n",
      "      BatchNorm2d-11        [-1, 128, 256, 256]             256\n",
      "             ReLU-12        [-1, 128, 256, 256]               0\n",
      "           Conv2d-13        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-14        [-1, 128, 256, 256]             256\n",
      "             ReLU-15        [-1, 128, 256, 256]               0\n",
      "       DoubleConv-16        [-1, 128, 256, 256]               0\n",
      "             Down-17        [-1, 128, 256, 256]               0\n",
      "        MaxPool2d-18        [-1, 128, 128, 128]               0\n",
      "           Conv2d-19        [-1, 256, 128, 128]         295,168\n",
      "      BatchNorm2d-20        [-1, 256, 128, 128]             512\n",
      "             ReLU-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-23        [-1, 256, 128, 128]             512\n",
      "             ReLU-24        [-1, 256, 128, 128]               0\n",
      "       DoubleConv-25        [-1, 256, 128, 128]               0\n",
      "             Down-26        [-1, 256, 128, 128]               0\n",
      "        MaxPool2d-27          [-1, 256, 64, 64]               0\n",
      "           Conv2d-28          [-1, 512, 64, 64]       1,180,160\n",
      "      BatchNorm2d-29          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-30          [-1, 512, 64, 64]               0\n",
      "           Conv2d-31          [-1, 512, 64, 64]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-33          [-1, 512, 64, 64]               0\n",
      "       DoubleConv-34          [-1, 512, 64, 64]               0\n",
      "             Down-35          [-1, 512, 64, 64]               0\n",
      "        MaxPool2d-36          [-1, 512, 32, 32]               0\n",
      "           Conv2d-37          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-39          [-1, 512, 32, 32]               0\n",
      "           Conv2d-40          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-41          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-42          [-1, 512, 32, 32]               0\n",
      "       DoubleConv-43          [-1, 512, 32, 32]               0\n",
      "             Down-44          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-45          [-1, 512, 64, 64]       1,049,088\n",
      "           Conv2d-46          [-1, 256, 64, 64]       2,359,552\n",
      "      BatchNorm2d-47          [-1, 256, 64, 64]             512\n",
      "             ReLU-48          [-1, 256, 64, 64]               0\n",
      "           Conv2d-49          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-50          [-1, 256, 64, 64]             512\n",
      "             ReLU-51          [-1, 256, 64, 64]               0\n",
      "       DoubleConv-52          [-1, 256, 64, 64]               0\n",
      "               Up-53          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-54        [-1, 256, 128, 128]         262,400\n",
      "           Conv2d-55        [-1, 128, 128, 128]         589,952\n",
      "      BatchNorm2d-56        [-1, 128, 128, 128]             256\n",
      "             ReLU-57        [-1, 128, 128, 128]               0\n",
      "           Conv2d-58        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-59        [-1, 128, 128, 128]             256\n",
      "             ReLU-60        [-1, 128, 128, 128]               0\n",
      "       DoubleConv-61        [-1, 128, 128, 128]               0\n",
      "               Up-62        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-63        [-1, 128, 256, 256]          65,664\n",
      "           Conv2d-64         [-1, 64, 256, 256]         147,520\n",
      "      BatchNorm2d-65         [-1, 64, 256, 256]             128\n",
      "             ReLU-66         [-1, 64, 256, 256]               0\n",
      "           Conv2d-67         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-68         [-1, 64, 256, 256]             128\n",
      "             ReLU-69         [-1, 64, 256, 256]               0\n",
      "       DoubleConv-70         [-1, 64, 256, 256]               0\n",
      "               Up-71         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-72         [-1, 64, 512, 512]          16,448\n",
      "           Conv2d-73         [-1, 64, 512, 512]          73,792\n",
      "      BatchNorm2d-74         [-1, 64, 512, 512]             128\n",
      "             ReLU-75         [-1, 64, 512, 512]               0\n",
      "           Conv2d-76         [-1, 64, 512, 512]          36,928\n",
      "      BatchNorm2d-77         [-1, 64, 512, 512]             128\n",
      "             ReLU-78         [-1, 64, 512, 512]               0\n",
      "       DoubleConv-79         [-1, 64, 512, 512]               0\n",
      "               Up-80         [-1, 64, 512, 512]               0\n",
      "           Conv2d-81          [-1, 3, 512, 512]             195\n",
      "          OutConv-82          [-1, 3, 512, 512]               0\n",
      "================================================================\n",
      "Total params: 14,789,059\n",
      "Trainable params: 14,789,059\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 3736.00\n",
      "Params size (MB): 56.42\n",
      "Estimated Total Size (MB): 3795.42\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]          36,928\n",
      "            Conv2d-2        [-1, 128, 128, 128]         147,584\n",
      "            Conv2d-3          [-1, 256, 64, 64]         590,080\n",
      "            Conv2d-4          [-1, 512, 32, 32]       2,359,808\n",
      "            Conv2d-5        [-1, 128, 128, 128]         147,584\n",
      "            Conv2d-6          [-1, 256, 64, 64]         590,080\n",
      "            Conv2d-7          [-1, 512, 32, 32]       2,359,808\n",
      "================================================================\n",
      "Total params: 6,231,872\n",
      "Trainable params: 6,231,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 88.00\n",
      "Params size (MB): 23.77\n",
      "Estimated Total Size (MB): 111.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "X = torch.randn(2, 3, 256*2, 256*2).to(device)\n",
    "out, hidden_features = unet_model(X)\n",
    "# (x1, x2, x3, x4, x5) = hidden_features\n",
    "fs = bridging_model(*hidden_features)\n",
    "B, C, H, W = fs[0].shape\n",
    "vs = [x.permute(0,2,3,1).reshape(B, H*W, C) for x in fs]\n",
    "\n",
    "x5 = hidden_features[4].permute(0,2,3,1).reshape(B, H*W, C)\n",
    "# print(x5.shape)\n",
    "\n",
    "feature_transformer_model = FeatureTransformer(data_dim=x5.shape[1:], hidden_d=512, n_heads=16, out_d=5).to(device)\n",
    "\n",
    "y_pred, out = feature_transformer_model(x5, vs[0], vs[1])\n",
    "\n",
    "summary(unet_model, (3, 512, 512), device=device.type)\n",
    "summary(bridging_model, [ x.shape[1:] for x in hidden_features ], device=device.type)\n",
    "# print(x5.shape[1:], vs[0].shape[1:], vs[1].shape[1:])\n",
    "# summary(feature_transformer_model, [ x5.shape[1:], vs[0].shape[1:], vs[1].shape[1:] ], device=device.type)\n",
    "\n",
    "# bridging_model = BridgingModel().to(device)\n",
    "# feature_transformer_model = FeatureTransformer().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
