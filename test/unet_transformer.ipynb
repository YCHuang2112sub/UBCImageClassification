{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REF\n",
    "\n",
    "- https://theaisummer.com/unet-architectures/\n",
    "- [CTRANSCNN](https://www.sciencedirect.com/science/article/pii/S0950705123007803)\n",
    "- [CrossViT]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import models\n",
    "from torchsummary import summary\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import validators\n",
    "\n",
    "import argparse\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(InConv, self).__init__()\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
    "        super(Up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2))\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super(Unet, self).__init__()\n",
    "        self.n_channels = in_channels\n",
    "        self.n_classes =  classes\n",
    "\n",
    "        self.inc = InConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256)\n",
    "        self.up2 = Up(512, 128)\n",
    "        self.up3 = Up(256, 64)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return x, (x1, x2, x3, x4, x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BridgingModel, self).__init__()\n",
    "\n",
    "        # self.mlp = nn.Sequential(\n",
    "        #     nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "        #     nn.GELU(),\n",
    "        #     nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        # )\n",
    "\n",
    "        self.trans1_1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, stride=2, padding=1)\n",
    "        )\n",
    "        self.trans1_2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
    "        )\n",
    "        self.trans1_3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        )\n",
    "        self.trans1_4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.trans2_2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, stride=2, padding=1)\n",
    "        )\n",
    "        self.trans2_3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, stride=2, padding=1)\n",
    "        )\n",
    "        self.trans2_4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2, x3, x4, x5):\n",
    "        # print(a1(x1).shape)\n",
    "        t1_1 = torch.cat( (x2[:, :64, :, :] + self.trans1_1(x1), x2[:, 64:, :, :] ), dim=1)\n",
    "        t1_2 = torch.cat( (x3[:, :128, :, :] + self.trans1_2(t1_1), x3[:, 128:, :, :] ), dim=1)\n",
    "        t1_3 = torch.cat( (x4[:, :256, :, :] + self.trans1_3(t1_2), x4[:, 256:, :, :] ), dim=1)\n",
    "        t1_4 = torch.cat( (x5[:, :512, :, :] + self.trans1_4(t1_3), x5[:, 512:, :, :] ), dim=1)\n",
    "\n",
    "        t2_2 = torch.cat( (t1_2[:, :128, :, :] + self.trans2_2(x2), t1_2[:, 128:, :, :] ), dim=1)\n",
    "        t2_3 = torch.cat( (t1_3[:, :256, :, :] + self.trans2_3(t2_2), t1_3[:, 256:, :, :] ), dim=1)\n",
    "        t2_4 = torch.cat( (t1_4[:, :512, :, :] + self.trans2_4(t2_3), t1_4[:, 512:, :, :] ), dim=1)\n",
    "\n",
    "        f1, f2 = t2_4, t1_4\n",
    "\n",
    "        return f1, f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_model = Unet(3, 3).to(device)\n",
    "# bridging_model = BridgingModel().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(unet_model, (3, ) + (256,256), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X = torch.randn(1, 3, 256, 256).to(device)\n",
    "# # out, hidden_features = unet_model(X)\n",
    "# # print(out.shape)\n",
    "\n",
    "# # (x1, x2, x3, x4, x5) = hidden_features\n",
    "# for feat in hidden_features:\n",
    "#     print(feat.shape)\n",
    "# # print(x1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 512])\n",
      "torch.Size([1, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "# X = torch.randn(1, 3, 256, 256).to(device)\n",
    "# out, hidden_features = unet_model(X)\n",
    "# # (x1, x2, x3, x4, x5) = hidden_features\n",
    "# fs = bridging_model(*hidden_features)\n",
    "# B, C, H, W = fs[0].shape\n",
    "# vs = [x.permute(0,2,3,1).reshape(B, H*W, C) for x in fs]\n",
    "\n",
    "# for v in vs:\n",
    "#     print(v.shape)\n",
    "\n",
    "# # torch.mean(torch.cat(vs)).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multihead_attn = nn.MultiheadAttention(128, 8, batch_first=True)\n",
    "# query = torch.randn(10, 32, 128)\n",
    "# key = query\n",
    "# value = torch.randn(10, 32, 128)\n",
    "# attn_output, attn_output_weights = multihead_attn(query, key, value, average_attn_weights=False)\n",
    "\n",
    "# print(attn_output.shape)\n",
    "# print(attn_output_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(MyViTBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        # self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "        \n",
    "        self.norm1_q = nn.LayerNorm(hidden_d)\n",
    "        self.norm1_k = nn.LayerNorm(hidden_d)\n",
    "        self.norm1_v = nn.LayerNorm(hidden_d)\n",
    "        self.multihead_attn = nn.MultiheadAttention(hidden_d, n_heads, batch_first=True)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "        )\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        # x = v\n",
    "        # out = x + self.mhsa(self.norm1(x))\n",
    "        q1, k1, v1 = self.norm1_q(q), self.norm1_k(k), self.norm1_v(v)\n",
    "        attn_output, attn_output_weights = self.multihead_attn(q1, k1, v1, average_attn_weights=False)\n",
    "        out = v + attn_output\n",
    "        \n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(nn.Module):\n",
    "    def __init__(self, data_dim=(256,512), hidden_d=512, n_heads=16, out_d=5) -> None:\n",
    "        super(FeatureTransformer, self).__init__()\n",
    "        \n",
    "        assert(len(data_dim) == 2)\n",
    "\n",
    "        self.data_dim = data_dim # (Length, embedding_dim)\n",
    "        self.seq_len, self.emb_dim = data_dim\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.out_d = out_d\n",
    "\n",
    "        # 1) Input transformation\n",
    "        self.linear_mapper = nn.Linear(data_dim[1], hidden_d)\n",
    "\n",
    "        # 2) Positional embedding\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.seq_len, hidden_d))\n",
    "        # self.pos_embeddings = self.get_positional_embeddings(self.seq_len, hidden_d)\n",
    "\n",
    "\n",
    "        # 3) Transformer encoder blocks\n",
    "        # self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "        self.block0 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block1 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block2 = MyViTBlock(hidden_d, n_heads)\n",
    "        self.block3 = MyViTBlock(hidden_d, n_heads)\n",
    "\n",
    "        # 4) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, hidden_d),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            # nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x5, v1, v2):\n",
    "        B_size = x5.shape[0]\n",
    "\n",
    "        tokens = self.linear_mapper(x5)\n",
    "        \n",
    "        # Adding classification token to the tokens\n",
    "        # tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        pos_embed = self.positional_embeddings.repeat(B_size, 1, 1)\n",
    "        out = tokens + pos_embed  \n",
    "        \n",
    "        # Transformer Blocks\n",
    "        # for block in self.blocks:\n",
    "        #     out2 = block(out, out, out)\n",
    "            \n",
    "        #     out = out2\n",
    "        x_stage1 = self.block0(out, v1, v1) # Q, K, V\n",
    "        x_stage2 = self.block1(x_stage1, x_stage1, x_stage1)\n",
    "        x_stage3 = self.block2(x_stage2, v2, v2)\n",
    "        x_stage4 = self.block3(x_stage3, x_stage3, x_stage3)\n",
    "\n",
    "        out = x_stage4\n",
    "            \n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "        \n",
    "        return self.mlp(out), out # Map to output dimension, output category distribution\n",
    "            \n",
    "    \n",
    "    def get_positional_embeddings(self, sequence_length, d):\n",
    "        result = torch.ones(sequence_length, d)\n",
    "        for i in range(sequence_length):\n",
    "            for j in range(d):\n",
    "                result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "        return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyViT(nn.Module):\n",
    "#     def __init__(self, chw=(1, 28, 28), n_patches=7, n_blocks=2, hidden_d=128, n_heads=2, out_d=10):\n",
    "#         # Super constructor\n",
    "#         super(MyViT, self).__init__()\n",
    "\n",
    "#         # Attributes\n",
    "#         self.chw = chw # (C, H, W)\n",
    "#         self.n_patches = n_patches\n",
    "#         self.n_blocks = n_blocks\n",
    "#         self.n_heads = n_heads\n",
    "#         self.hidden_d = hidden_d\n",
    "\n",
    "#         assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "#         assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "#         self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "#         # 1) Linear mapper\n",
    "#         self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "#         self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "#         # 2) Learnable classifiation token\n",
    "#         self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "        \n",
    "#         # # 3) Positional embedding\n",
    "#         # self.positional_embeddings = nn.Parameter(torch.tensor(self.get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))\n",
    "#         # self.positional_embeddings.requires_grad = False\n",
    "        \n",
    "        \n",
    "#         # 3) Positional embedding\n",
    "#         self.register_buffer('positional_embeddings', self.get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "        \n",
    "#         # 4) Transformer encoder blocks\n",
    "#         self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "           \n",
    "#         # 5) Classification MLPk\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(self.hidden_d, out_d),\n",
    "#             nn.Softmax(dim=-1)\n",
    "#         )\n",
    "        \n",
    "\n",
    "#     def forward(self, images):\n",
    "#         n, c, h, w = images.shape\n",
    "#         patches = self.patchify(images, self.n_patches)\n",
    "#         # print(patches)\n",
    "#         tokens = self.linear_mapper(patches)\n",
    "        \n",
    "#         # Adding classification token to the tokens\n",
    "#         tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "        \n",
    "#         # Adding positional embedding\n",
    "#         pos_embed = self.positional_embeddings.repeat(n, 1, 1)\n",
    "#         out = tokens + pos_embed  \n",
    "        \n",
    "#         # Transformer Blocks\n",
    "#         for block in self.blocks:\n",
    "#             out2 = block(out, out, out)\n",
    "            \n",
    "#             out = out2\n",
    "            \n",
    "#         # Getting the classification token only\n",
    "#         out = out[:, 0]\n",
    "        \n",
    "#         return self.mlp(out), out # Map to output dimension, output category distribution\n",
    "            \n",
    "\n",
    "#     def patchify(self, images, n_patches):\n",
    "#         assert len(images.shape) == 4\n",
    "#         b,c,h,w = images.shape\n",
    "\n",
    "#         size_h = int(h/n_patches) # patch size\n",
    "#         stride_h = size_h # patch stride\n",
    "#         size_w = int(w/n_patches) # patch size\n",
    "#         stride_w = size_w # patch stride\n",
    "#         x = images.permute((0,2,3,1))\n",
    "#         x2 = x.unfold(1, size_h, stride_h).unfold(2, size_w, stride_w).flatten(start_dim=1, end_dim=2)\n",
    "#         # print(x2.shape, \"x2 pachify\") # x2.shape = (b,p**2, c,h,w)\n",
    "#         x3 = x2.permute((0,1,3,4,2)).flatten(start_dim=2, end_dim=4)\n",
    "#         # print(x3.shape, \"x3 pachify\") # x3.shape = (b,p**2,h*w*c)\n",
    "        \n",
    "#         return x3\n",
    "\n",
    "#     def get_positional_embeddings(self, sequence_length, d):\n",
    "#         result = torch.ones(sequence_length, d)\n",
    "#         for i in range(sequence_length):\n",
    "#             for j in range(d):\n",
    "#                 result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "#         return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd() + '/../')\n",
    "from lib.network_architecture.unet_transformer_01 import MyViTBlock, FeatureTransformer, Unet, BridgingModel, \\\n",
    "                                                         get_unet_transformer_model_output\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda for inference\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = Unet(3, 3).to(device)\n",
    "bridging_model = BridgingModel().to(device)\n",
    "feature_transformer_model = FeatureTransformer().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0450, -0.0544, -0.0116,  0.2107,  0.0114]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward0>) tensor([3], device='cuda:0')\n",
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 3, 256, 256).to(device)\n",
    "# out, hidden_features = unet_model(X)\n",
    "# # (x1, x2, x3, x4, x5) = hidden_features\n",
    "# fs = bridging_model(*hidden_features)\n",
    "# B, C, H, W = fs[0].shape\n",
    "# vs = [x.permute(0,2,3,1).reshape(B, H*W, C) for x in fs]\n",
    "\n",
    "\n",
    "# for v in vs:\n",
    "#     print(v.shape)\n",
    "\n",
    "\n",
    "# x5 = hidden_features[4].permute(0,2,3,1).reshape(B, H*W, C)\n",
    "# print(x5.shape)\n",
    "\n",
    "# y_pred, out = feature_transformer_model(x5, vs[0], vs[1])\n",
    "\n",
    "y_pred, out, unet_out = get_unet_transformer_model_output(X, unet_model, bridging_model, feature_transformer_model)\n",
    "\n",
    "print(y_pred, torch.argmax(y_pred, dim=-1))\n",
    "print(unet_out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 512, 512]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 512, 512]             128\n",
      "              ReLU-3         [-1, 64, 512, 512]               0\n",
      "            Conv2d-4         [-1, 64, 512, 512]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 512, 512]             128\n",
      "              ReLU-6         [-1, 64, 512, 512]               0\n",
      "        DoubleConv-7         [-1, 64, 512, 512]               0\n",
      "            InConv-8         [-1, 64, 512, 512]               0\n",
      "         MaxPool2d-9         [-1, 64, 256, 256]               0\n",
      "           Conv2d-10        [-1, 128, 256, 256]          73,856\n",
      "      BatchNorm2d-11        [-1, 128, 256, 256]             256\n",
      "             ReLU-12        [-1, 128, 256, 256]               0\n",
      "           Conv2d-13        [-1, 128, 256, 256]         147,584\n",
      "      BatchNorm2d-14        [-1, 128, 256, 256]             256\n",
      "             ReLU-15        [-1, 128, 256, 256]               0\n",
      "       DoubleConv-16        [-1, 128, 256, 256]               0\n",
      "             Down-17        [-1, 128, 256, 256]               0\n",
      "        MaxPool2d-18        [-1, 128, 128, 128]               0\n",
      "           Conv2d-19        [-1, 256, 128, 128]         295,168\n",
      "      BatchNorm2d-20        [-1, 256, 128, 128]             512\n",
      "             ReLU-21        [-1, 256, 128, 128]               0\n",
      "           Conv2d-22        [-1, 256, 128, 128]         590,080\n",
      "      BatchNorm2d-23        [-1, 256, 128, 128]             512\n",
      "             ReLU-24        [-1, 256, 128, 128]               0\n",
      "       DoubleConv-25        [-1, 256, 128, 128]               0\n",
      "             Down-26        [-1, 256, 128, 128]               0\n",
      "        MaxPool2d-27          [-1, 256, 64, 64]               0\n",
      "           Conv2d-28          [-1, 512, 64, 64]       1,180,160\n",
      "      BatchNorm2d-29          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-30          [-1, 512, 64, 64]               0\n",
      "           Conv2d-31          [-1, 512, 64, 64]       2,359,808\n",
      "      BatchNorm2d-32          [-1, 512, 64, 64]           1,024\n",
      "             ReLU-33          [-1, 512, 64, 64]               0\n",
      "       DoubleConv-34          [-1, 512, 64, 64]               0\n",
      "             Down-35          [-1, 512, 64, 64]               0\n",
      "        MaxPool2d-36          [-1, 512, 32, 32]               0\n",
      "           Conv2d-37          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-38          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-39          [-1, 512, 32, 32]               0\n",
      "           Conv2d-40          [-1, 512, 32, 32]       2,359,808\n",
      "      BatchNorm2d-41          [-1, 512, 32, 32]           1,024\n",
      "             ReLU-42          [-1, 512, 32, 32]               0\n",
      "       DoubleConv-43          [-1, 512, 32, 32]               0\n",
      "             Down-44          [-1, 512, 32, 32]               0\n",
      "  ConvTranspose2d-45          [-1, 512, 64, 64]       1,049,088\n",
      "           Conv2d-46          [-1, 256, 64, 64]       2,359,552\n",
      "      BatchNorm2d-47          [-1, 256, 64, 64]             512\n",
      "             ReLU-48          [-1, 256, 64, 64]               0\n",
      "           Conv2d-49          [-1, 256, 64, 64]         590,080\n",
      "      BatchNorm2d-50          [-1, 256, 64, 64]             512\n",
      "             ReLU-51          [-1, 256, 64, 64]               0\n",
      "       DoubleConv-52          [-1, 256, 64, 64]               0\n",
      "               Up-53          [-1, 256, 64, 64]               0\n",
      "  ConvTranspose2d-54        [-1, 256, 128, 128]         262,400\n",
      "           Conv2d-55        [-1, 128, 128, 128]         589,952\n",
      "      BatchNorm2d-56        [-1, 128, 128, 128]             256\n",
      "             ReLU-57        [-1, 128, 128, 128]               0\n",
      "           Conv2d-58        [-1, 128, 128, 128]         147,584\n",
      "      BatchNorm2d-59        [-1, 128, 128, 128]             256\n",
      "             ReLU-60        [-1, 128, 128, 128]               0\n",
      "       DoubleConv-61        [-1, 128, 128, 128]               0\n",
      "               Up-62        [-1, 128, 128, 128]               0\n",
      "  ConvTranspose2d-63        [-1, 128, 256, 256]          65,664\n",
      "           Conv2d-64         [-1, 64, 256, 256]         147,520\n",
      "      BatchNorm2d-65         [-1, 64, 256, 256]             128\n",
      "             ReLU-66         [-1, 64, 256, 256]               0\n",
      "           Conv2d-67         [-1, 64, 256, 256]          36,928\n",
      "      BatchNorm2d-68         [-1, 64, 256, 256]             128\n",
      "             ReLU-69         [-1, 64, 256, 256]               0\n",
      "       DoubleConv-70         [-1, 64, 256, 256]               0\n",
      "               Up-71         [-1, 64, 256, 256]               0\n",
      "  ConvTranspose2d-72         [-1, 64, 512, 512]          16,448\n",
      "           Conv2d-73         [-1, 64, 512, 512]          73,792\n",
      "      BatchNorm2d-74         [-1, 64, 512, 512]             128\n",
      "             ReLU-75         [-1, 64, 512, 512]               0\n",
      "           Conv2d-76         [-1, 64, 512, 512]          36,928\n",
      "      BatchNorm2d-77         [-1, 64, 512, 512]             128\n",
      "             ReLU-78         [-1, 64, 512, 512]               0\n",
      "       DoubleConv-79         [-1, 64, 512, 512]               0\n",
      "               Up-80         [-1, 64, 512, 512]               0\n",
      "           Conv2d-81          [-1, 3, 512, 512]             195\n",
      "          OutConv-82          [-1, 3, 512, 512]               0\n",
      "================================================================\n",
      "Total params: 14,789,059\n",
      "Trainable params: 14,789,059\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 3736.00\n",
      "Params size (MB): 56.42\n",
      "Estimated Total Size (MB): 3795.42\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]          36,928\n",
      "            Conv2d-2          [-1, 128, 64, 64]         147,584\n",
      "            Conv2d-3          [-1, 256, 32, 32]         590,080\n",
      "            Conv2d-4          [-1, 512, 16, 16]       2,359,808\n",
      "            Conv2d-5          [-1, 128, 64, 64]         147,584\n",
      "            Conv2d-6          [-1, 256, 32, 32]         590,080\n",
      "            Conv2d-7          [-1, 512, 16, 16]       2,359,808\n",
      "================================================================\n",
      "Total params: 6,231,872\n",
      "Trainable params: 6,231,872\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 22.00\n",
      "Params size (MB): 23.77\n",
      "Estimated Total Size (MB): 45.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "X = torch.randn(2, 3, 256, 256).to(device)\n",
    "out, hidden_features = unet_model(X)\n",
    "# (x1, x2, x3, x4, x5) = hidden_features\n",
    "fs = bridging_model(*hidden_features)\n",
    "B, C, H, W = fs[0].shape\n",
    "vs = [x.permute(0,2,3,1).reshape(B, H*W, C) for x in fs]\n",
    "\n",
    "x5 = hidden_features[4].permute(0,2,3,1).reshape(B, H*W, C)\n",
    "# print(x5.shape)\n",
    "\n",
    "y_pred, out = feature_transformer_model(x5, vs[0], vs[1])\n",
    "\n",
    "summary(unet_model, (3, 512, 512), device=device.type)\n",
    "summary(bridging_model, [ x.shape[1:] for x in hidden_features ], device=device.type)\n",
    "# print(x5.shape[1:], vs[0].shape[1:], vs[1].shape[1:])\n",
    "# summary(feature_transformer_model, [ x5.shape[1:], vs[0].shape[1:], vs[1].shape[1:] ], device=device.type)\n",
    "\n",
    "# bridging_model = BridgingModel().to(device)\n",
    "# feature_transformer_model = FeatureTransformer().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
